{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secondary-singapore",
   "metadata": {},
   "source": [
    "### **Project Proposal** \n",
    "#### Neocognitron, Team 4 - Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-boundary",
   "metadata": {},
   "source": [
    "##### Zernab Saeed, Daniel Sindell, Pratap Karki, Michael Kwarteng and Alex Lopez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-equity",
   "metadata": {},
   "source": [
    "### Aim\n",
    "##### The aim of this project is to design a deep learning neural network that analyzes an image and generates a caption that describes the image. Upon research, we have discovered that this is called “Image Captioning”. Initially we were motivated by the idea to describe the action occurring within an image but given the time constraints it seems feasible to develop and train a neural network that identifies types of objects within an image and generates a meaningful caption, limited to a few words for simplicity. \n",
    "### Dataset\n",
    "##### We hope to use a dataset that consists of images that can be used as validation input and corresponding captions as output. The “Open Images V6” dataset we came across, has about 9 million images that are annotated with labels and visual relationships that include a training set, validation set and test set. We can modify this dataset to make it easy to work with as we learn how to prepare this data. This is a potential dataset and can be subject to change if we feel that the dataset is proving to be more complicated than we imagined. Another dataset we can leverage is the “Flickr8k” dataset. This is a much smaller dataset with 8 thousand images that are paired with different captions regarding the descriptions of the photograph. The captions with these images can be modified to create a much simpler network. Our group can prepare this dataset.\n",
    "### Architecture\n",
    "##### Image Captioning can get convoluted so we had to explore a range of different architectures we could use. Our research led us to believe that using a “encoder-decoder recurrent neural network” model is going to be the best approach. The encoder analyzes a photograph using an internal representation and encodes its findings into a vector. This encoder takes the form of a Convolutional Neural Network and the vector is going to be an input into a Recurrent Neural Network, which works as a decoder. We plan to build this CNN and RNN using TensorFlow. The image data can be fed to the neural network either directly in the RNN or in a layer following the RNN which gives rise to the “inject and merge” architectures. The inject model produces the next word in the caption by combining the encoded image with each word. The merge model encodes the image and definition separately, which is then decoded to produce the next word in the caption. The merge model seems to be more successful so we will most likely be using that approach. Brownlee discusses these approaches in his “Deep Learning for Natural Language Processing” Blog, which was an essential part in helping us frame our proposal.\n",
    "### Evaluation\n",
    "##### Since our image captioning neural network will be generating captions to identify images, we can use this aspect to test our network. The dataset we intend to use for validation and training includes a testing data set. These will be images that the neural network has never seen before. We plan on using “The Bilingual Evaluation Understudy Score”, BLEU, to evaluate our model’s generated caption to reference captions in order to test accuracy. An implementation of the BLEU score can be found in the “Python Natural Language Toolkit library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-strengthening",
   "metadata": {},
   "source": [
    "### References \n",
    "##### [1] https://github.com/openimages/dataset\n",
    "##### [2] https://github.com/jbrownlee/Datasets\n",
    "##### [3] https://storage.googleapis.com/openimages/web/factsfigures.html\n",
    "##### [4] Brownlee, Jason. “How to Develop a Deep Learning Photo Caption Generator from Scratch.” Machine Learning Mastery, Jason Brownlee, 26 June 2019, https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-yield",
   "metadata": {},
   "source": [
    "##### [5] Kuznetsova, Alina, et al. “The Open Images Dataset V4.” International Journal of Computer Vision, Mar. 2020, pp. 1956–1981., doi:10.1007/s11263-020-01316-z. \n",
    "##### [6] Brownlee, Jason. “Caption Generation with the Inject and Merge Encoder-Decoder Models.” Machine Learning Mastery, 26 Dec. 2017, https://machinelearningmastery.com/caption-generation-inject-merge-architectures-encoder-decoder-model/.\n",
    "##### [7] Tanti, M., Gatt, A., & CAMILLERI, K. (2018). Where to put the image in an image caption generator. Natural Language Engineering, 24(3), 467-489. doi:10.1017/S1351324918000098\n",
    "##### [8] Tanti, M., Gatt, A., & Camilleri, K. (2017). What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ArXiv, abs/1708.02043."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
